---
layout: post
title: El Ecosistema Hadoop.
date: '2010-09-29T22:19:00.002+01:00'
author: Marc de Palol
tags:
- pig
- hbase
- avro
- flume
- hadoop
- hive
- dfs
- zookeeper
modified_time: '2010-09-29T22:22:47.059+01:00'
blogger_id: tag:blogger.com,1999:blog-6541464271719475452.post-952207965840165568
blogger_orig_url: http://distribuint.blogspot.com/2010/09/el-ecosistema-hadoop.html
---

<span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Hadoop es sin duda uno de los proyectos que más acogida últimamente ha tenido entre la comunidad de software libre y es también uno de los responsables del auge de la ciencia de datos.&nbsp;</span><br /><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Hadoop es un framework, aunque hoy en día se puede hablar de todo un ecosistema de proyectos alrededor del nucleo, vamos a intentar explicar en este post los distintos componentes de forma senzilla:&nbsp;</span><br /><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"><br /></span><br /><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">vamos a empezar por el principio,&nbsp;</span><br /><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"><br /></span><br /><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif; font-size: x-large;">Hadoop</span><br /><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Con Hadoop nos referimos a la parte central del sistema, que es el sistema de ficheros distribuido (HDFS) y el motor de Map/Reduce. Si echamos un vistazo a la </span><a href="http://hadoop.apache.org/"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">página del proyecto</span></a><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"> veremos que hay 3 subproyectos, calma y tranquilidad!, a efectos prácticos van totalmente juntos y es una separación que se hizo por temas de código fuente:</span><br /><ul><li><i><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Hadoop MapReduce.</span></i></li><li><i><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">HDFS.</span></i></li><li><i><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Hadoop - Commons.</span></i></li></ul><div><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Una vez aclarado el tema de la separación. Qué es Hadoop? Pues es la combinación de:</span></div><div><ul><li><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">un sistema de ficheros distribuido (diferentes discos duros en diferentes máquines se ven como uno solo) (</span><b><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">HDFS)</span></b></li><li><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">de un modelo de programación que se llama Map/Reduce, con su API en Java (</span><b><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Hadoop MapReduce</span></b><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">)</span></li><li><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">y un componente software que ejecuta los programas que hemos programado con la API en un cluster, usando asi el sistema de ficheros distribuido.&nbsp;</span></li><li><i><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">y una serie de classes comunas</span></span><b><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"> </span></b><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">(</span></span></i><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"><b>Hadoop - Commons</b>)</span></li></ul></div><div><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Esto es lo básico que debemos instalar en el clúster para obtener la funcionalidad. De este modo podemos meter datos en el sistema de ficheros y tratarlos con nuestros programas programados con el API de Hadoop.&nbsp;</span><br /><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"><br /></span></div><div><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Vamos a ver qué más hay:</span><br /><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"><br /></span></div><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif; font-size: x-large;">HBase</span><br /><div><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Una vez tenemos los datos procesados en Hadoop seguramente nos interesará tenerlos fácilmente accesibles para diferentes tipos de consultas, algo así como una base de datos con los resultados de Hadoop. Aquí es donde entra </span><a href="http://hbase.apache.org/"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">HBase</span></a><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">.</span></div><div><i> </i><i><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"> </span></i><br /><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">HBase es una implementación libre del Bigtable de Google, es una de las famosas bases de datos NoSql. Como el nombre indica, no usa Sql, sinó que tiene una API, además el model de datos no es tabular, sinó que está basado en columnas de n&nbsp;dimensiones. Ya hablé de este modelo de datos en </span><a href="http://distribuint.blogspot.com/2009/05/bigtable.html"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">un post anterior</span></a><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">.</span><br /><i> </i><br /><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">A efectors prácticos, se tratan los datos en Hadoop mediante Map/Reduce y se escriben en HBase (importante saber que HBase está totalmente integrado con Hadoop, y que por lo tanto, HBase entiende perfectamente el sistema de ficheros de Hadoop asi como el formato de los ficheros), que puede estar en el mismo clúster que Hadoop o en otro, para que otras aplicaciones puedan utilizar los datos en tiempo real.</span><br /><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"><br /></span></div><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"><span class="Apple-style-span" style="font-size: x-large;">Zookeeper</span></span><br /><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Tanto HBase como Hadoop son sistemas distribuidos en los que el sistema debe controlar y saber en cada momento lo que está pasando en procesos que están en otras máquinas. Si hemos programado alguna vez este tipo de sistemas sabremos que eso no es una tarea fácil, nos vamos a encontrar todo tipos de problemas.&nbsp;</span><br /><i> </i><i><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"> </span></i><br /><a href="http://hadoop.apache.org/zookeeper/"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Zookeeper</span></a><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"> aparece justamente para solucionarlos. Igual que HBase es la implementación libre de un paper de google, en este caso de </span><a href="http://labs.google.com/papers/chubby.html"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Google Chubby</span></a><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">.&nbsp;</span><br /><i> </i><br /><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Básicamente es un sistema de locks&nbsp;distribuido y de consenso (utilizando </span><a href="http://en.wikipedia.org/wiki/Paxos_algorithm"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Paxos</span></a><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">), no hace mucho los ingenieros de LinkedIn lo usaron para un sistema interno y hicieron un post muy interesante, en el que utilizaron la siguiente definición, muy acertada en mi opinión:&nbsp;</span><br /><blockquote><i><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"><span class="Apple-style-span" style="font-size: small;">&nbsp;ZK&nbsp;has a very simple, file system like API. One can create a path, set the value of a path, read the value of a path, delete a path, and list the children of a path. ZK does a couple of more interesting things:&nbsp;</span></span></i></blockquote><blockquote><i><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"><span class="Apple-style-span" style="font-size: small;">(a) one can register a watcher on a path and get notified when the children of a path or the value of a path is changed,&nbsp;</span></span></i></blockquote><blockquote><i><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"><span class="Apple-style-span" style="font-size: small;"> (b) a path can be created as ephemeral, which means that if the client that created the path is gone, the path is automatically removed by the ZK server.</span></span></i></blockquote><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">de:&nbsp;</span><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"><a href="http://sna-projects.com/blog/2010/08/zookeeper-experience/">http://sna-projects.com/blog/2010/08/zookeeper-experience/</a></span><br /><i> </i><br /><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Asi que ya tenemos otra pieza del puzzle, si hay que sincronizar sistemas distribuidos (que utilizen o no Hadoop), podemos utilizar ZooKeeper.</span><br /><i> </i><br /><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Ahora toca el turno de </span><b><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">hive</span></b><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">, pero atención porqué después viene </span><b><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">pig</span></b><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">&nbsp;y aunque vamos a ver que son bastante diferentes, los dos proyectos pretenden solucionar un problema muy similar:</span><br /><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"><br /></span><br /><i> </i><i><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"> </span></i><br /><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"><span class="Apple-style-span" style="font-size: x-large;">Hive</span></span><br /><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">La principal incomodidad&nbsp;que tiene Hadoop es que para hacer una simple consulta de datos hay que escribir un programa completo en Java, y esto, no es rápido, es un poco difícil de debuggear&nbsp;y puede llegar a ser frustrante.&nbsp;</span><br /><i> </i><br /><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Pongamos el caso en que tenemos 500 Gb de datos del tipo:&nbsp;</span><br /><i> </i><br /><span class="Apple-style-span" style="font-size: small;"><span class="Apple-style-span" style="font-family: 'Courier New', Courier, monospace;">Key: Integer</span></span><br /><span class="Apple-style-span" style="font-size: small;"><span class="Apple-style-span" style="font-family: 'Courier New', Courier, monospace;">Value: Objecto(Id: Integer, valor1: Integer, valor2: Integer)</span></span><br /><i> </i><br /><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">y que en un momento dado queremos un listado de todos los pares Key/Value dónde valor2 &gt; 500.&nbsp;</span><br /><i> </i><br /><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Si los datos estuvieran guardados en una base de datos podriamos hacer una senzilla consulta Sql con un where. En Hadoop tocaría escribir un programa en Java. Pues bien, </span><a href="http://hadoop.apache.org/hive/"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Hive</span></a><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"> al rescate.&nbsp;</span><br /><i> </i><br /><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Se trata básicamente de una infraestructura de data warehousing&nbsp;encima de Hadoop. Y esto que significa? pues básicamente se trata de crear unos metadatos encima de los directorios de HDFS describiendo el formato de los ficheros, asi pues creamos una estructura tabular virtual encima de Hadoop.&nbsp;Hive también tiene un intérprete de comandas sql, por lo tanto podemos escribir una consulta sql de toda la vida en la command line, luego hive coge el sql, lo convierte automáticamente en jobs Map/Reduce&nbsp;y estos son ejecutados de forma transparente en el clúster como jobs normales.&nbsp;</span><br /><i><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"><br /></span> </i><br /><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Si quereis más información, anteriormente hice </span><a href="http://distribuint.blogspot.com/2009/07/aquest-post-parlo-de-hive-una-eina-que.html"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">un post</span></a><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"> hablando de hive con más detalle.</span><br /><i><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"><br /></span> </i><br /><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"><span class="Apple-style-span" style="font-size: x-large;">Pig</span></span><br /><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Otro modo de ver el anterior problema es: en vez de escribir un programa en Java para hacer una consulta de unos datos, voy a hacerlo con nserta aquí tu lenguaje de scripting favorito&gt;. Hadoop tiene como parte del Hadoop Core una opción de streaming. Básicamente te permite especificar como parámetro dos scripts (uno que hace de mapper y otro que hace de reducer) que leen y escriben por la entrada y salida estándar. Esto es práctico pero te limita a unos programas muy sencillos y poco aplicables a la vida real.&nbsp;</span><br /><i><i><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"><br /></span><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"> Pues bien, </span></span><a href="http://hadoop.apache.org/pig/"><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Pig</span></span></a><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"> es un lenguage de programación (tipo scripting) para Hadoop, y</span></span></i><i><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">&nbsp;por lo que estoy viendo parece que tiene una muy buena aceptación en la comunidad Hadoop (pero que </span></span><b><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">muy</span></span></b><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">&nbsp;buena aceptación).</span></span><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"><br /></span></span><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">  <br /></span> <span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"><span class="Apple-style-span" style="font-size: x-large;">Avro</span></span></span><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"><br />Una de los puntos fuertes del ecosistema Hadoop es que facilita trabajar con grandes cantidades de datos. &nbsp;Estos datos están guardados en el HDFS, pero cómo? pues se pueden guardar en diferentes formatos, el más simple es el de texto plano, pero normalmente se recomienda usar serialización.&nbsp;</span></span></i></i><br /><i><i><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"><br /></span></span></i></i><br /><i><i><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Qué es la serialización? pues se trata de como almacenamos la información internamente de un objeto a disco (o memoria), podemos </span><a href="http://es.wikipedia.org/wiki/Serializaci%C3%B3n"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">leer más en la wikipedia.</span></a><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">&nbsp;Para Hadoop lo más interesante son los formatos binarios, que son los más rápidos y comprimidos, el problema es que un humano no los puede leer, pero en este caso no nos importa.&nbsp;</span></span></i></i><br /><i><i><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"><br /></span></span></i></i><br /><i><i><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Tenemos diferentes formatos:</span></span><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">&nbsp;</span></span></i></i><br /><i><i></i></i><br /><i><i></i></i><br /><i><i><ul><li><span class="Apple-style-span" style="font-style: normal;"><a href="http://code.google.com/apis/protocolbuffers/"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Protocol Buffers</span></a><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">, creado por Google.</span></span></li><li><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"><a href="http://incubator.apache.org/thrift/">Thrift</a>, creado por Facebook.</span></span></li></ul><div><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">y otros dos, creados por los programadors de Hadoop:</span></span></div><div><ul><li><a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/record/package-summary.html#package_description"><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">RecordIO</span></span></a><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">: Hasta hace pronto el formato de serialización de facto en Hadoop, bastante interesante en su implementación, pero con algunos problemas en temas de mantenimiento a largo plazo (los objetos generados son estáticos, esto significa que si modificamos las características del objeto (introducimos un nuevo campo) el objeto nuevo será incompatible con el viejo). Si echamos una ojeada a la documentación veremos que está deprecated by... si señor:</span></span></li><li><span class="Apple-style-span" style="font-style: normal;"><a href="http://avro.apache.org/docs/current/"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Avro</span></a><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">.</span></span></li></ul><div><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Tiene unas características muy interesantes, básicamente:&nbsp;</span></span></div><div><ul><li><span class="Apple-style-span" style="font-style: normal;"><i><i><div style="display: inline !important;"><div style="display: inline !important;"><div style="display: inline !important;"><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Permite almacenar estructuras de datos simples y complejas (desde números hasta objectos con listas y maps)</span></span></div></div></div></i></i></span></li><li><span class="Apple-style-span" style="font-style: normal;"><i><i><div style="display: inline !important;"><div style="display: inline !important;"><div style="display: inline !important;"><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Es compacto, rápido y binario. (Esto no es mucha novedad)</span></span></div></div></div></i></i></span></li><li><span class="Apple-style-span" style="font-style: normal;"><i><i><div style="display: inline !important;"><div style="display: inline !important;"><div style="display: inline !important;"><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Tiene un format de fichero propio.</span></span></div></div></div></i></i></span></li><li><span class="Apple-style-span" style="font-style: normal;"><i><i><div style="display: inline !important;"><div style="display: inline !important;"><div style="display: inline !important;"><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Tiene una especificación de RPC. (como Thrift)</span></span></div></div></div></i></i></span></li><li><span class="Apple-style-span" style="font-style: normal;"><i><i><div style="display: inline !important;"><div style="display: inline !important;"><div style="display: inline !important;"><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Y lo más interesante: El esquema de serialización (el índice que dice que tipo de datos está en cada posición del fichero) está incluido en el fichero en si. Esto permite que la generación de código por parte del precompilador sea opcional y lo que es más importante, hace que podamos canviar la estructura interna de los objectos almacenados sin perder la compatibilidad con ficheros viejos.</span></span></div></div></div></i></i></span></li></ul><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Por lo tanto, si trabajais con Hadoop, a serializar los datos con Avro desde ya.</span></span><br /><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"><br /></span></span></div></div></i><i><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"> </span><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"><span class="Apple-style-span" style="font-size: x-large;">Flume</span></span></span></i><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"><br /></span></span><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"> <span class="Apple-style-span" style="font-style: normal;"></span></span><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">The "new kid in town". </span></span><a href="http://github.com/cloudera/flume"><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Flume</span></span></a><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"> fue liberado por Cloudera no hace ni medio año y ya ha tenido una gran adopción.&nbsp;</span></span><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"><br /></span></span><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"> <span class="Apple-style-span" style="font-style: normal;"></span> </span><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"><br /></span></span><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"> </span><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">La idea es muy sencilla, se trata de una pipe al HDFS.&nbsp;</span></span><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"><br /></span></span><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"> <span class="Apple-style-span" style="font-style: normal;"></span> </span> <span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">Lógicamente para trabajar con Hadoop debemos tener los datos en el HDFS, y tenemos diferentes herramientas para poner los ficheros allí, el problema es que cuando tenemos diferentes fuentes de datos (el syslog de muchos servidores web por ejemplo) hay que construir una infraestructura que se asegure que los logs se van subiendo periódicamente y que no falte ninguno. Pues bien, Flume es exactamente esto. Además utiliza ZooKeeper, por lo tanto es:</span></span><br /><br /><ul><li><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">&nbsp;tolerante a fallos ( si un nodo cae de la red, cuando se vuelva a conectar sabrá que tiene que enviar de nuevo ),&nbsp;</span></span></li><li><span class="Apple-style-span" style="font-style: normal;"><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;">tiene configuración distribuida (puede reconfigurar los nodos cliente a partir de un nodo máster)&nbsp;</span></span></li></ul><div><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"><span class="Apple-style-span" style="font-style: normal;"><br /></span></span></div></i><span class="Apple-style-span" style="font-family: 'Trebuchet MS', sans-serif;"> Y no se acaba aquí! hay más, mucho más en Hadoop, pero creo que estos son los proyectos más interesantes y que estás más ligados a Hadoop.<br /><br />Echáis en falta alguno?</span>