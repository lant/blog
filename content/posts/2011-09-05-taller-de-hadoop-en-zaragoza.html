---
layout: post
title: Taller de Hadoop en Zaragoza
date: '2011-09-05T07:54:00.003+01:00'
author: Marc de Palol
tags:
- ec2
- hadoop
- map/reduce
- hdfs
- taller
modified_time: '2011-09-05T07:54:30.219+01:00'
blogger_id: tag:blogger.com,1999:blog-6541464271719475452.post-7346026972264658639
blogger_orig_url: http://distribuint.blogspot.com/2011/09/taller-de-hadoop-en-zaragoza.html
---

El día 31 de Agosto dí una charla en Zaragoza sobre Hadoop, parte de los talleres que organiza <a href="http://cachirulovalley.com/">Cachirulo Valley</a>. Hoy quería subir las transparencias a <a href="http://www.slideshare.net/marc.de.palol">slideshare</a> cómo de costumbre, pero mirándolas me he dado cuenta de que no tiene mucho sentido ya que parece ser que mis presentaciones son muy gráficas. Así que he decidido hacer un pequeño resumen de la charla y colgarlo aquí. Espero que sirva para aclarar conceptos y como pequeña introducción a Hadoop.<br /><br /><span class="Apple-style-span" style="font-size: large;">Qué es Hadoop y para qué sirve ?&nbsp;</span><br />Primero vamos a poner un problema, y luego alguien lo va a solucionar muy bien.<br />Por allí al 200X Google tenía un problema, y este problema era básicamente una cantidad gigante de datos con que trabajar. Las soluciones que había por aquel entonces o bien no eran lo suficientemente potentes o eran demasiado caras, así que teniendo los recursos, la gente de Google diseñó su propia solución, que finalmente implementó, provó, puso en producción y finalmente, explicó a la comunidad mediante una serie de <a href="http://research.google.com/pubs/papers.html">papers</a>:<br /><ul><li><b>MapReduce: Simplified Data Processing on Large Clusters </b>del que ya hablé en este mismo blog.</li><li><b>The Google File System</b>.</li></ul>Poco después, Doug Cutting, que estaba participando en un proyecto llamado <a href="http://nutch.apache.org/">nutch</a>&nbsp;tuvo el mismo problema, qué hacer con tantos datos? encontró el paper de google y se puso a implementarlo, así de fácil, y así nació Hadoop, la implementación libre de los dos papers de Google.<br /><br />Aquí, igual que en el taller me voy a servir de una muy buena definición de Hadoop, de Parand Tony Darugar:<br /><blockquote>"<i>Flexible infrastructure for large scale computational and data processing on a network of commodity hardware</i>".</blockquote>Vamos a analizar la frase:<br /><ol><li><i>Data processing: </i>Hadoop no está pensado para problemas matemáticos, no estamos calculando simulaciones, no estamos calculando grafos. Hadoop es para procesar datos. Si no tienes muchos datos de entrada te estás equivocando de Framework.&nbsp;</li><li><i>Network:</i>&nbsp;I por qué en una red de ordenadores? pues porqué estás analizando tantos datos que no te caben en una sola máquina. Si los datos te caben en una sola&nbsp;máquina te estás equivocando de Framework.&nbsp;</li><li><i>Large Scale</i>: Muchos datos y muchas máquinas.&nbsp;</li><li><i>Commodity Hardware</i>: Tenemos una red de máquinas. El hardware va a fallar, estadísticamente los discos duros van a estropear-se, la RAM se averiará y las placas base se van a fundir. Si tenemos mucho hardware tenemos muchas posibilidades de que hayan desgracias, así que Hadoop está preparado para correr en máquinas "baratas". Si el hardware <b>se va </b>&nbsp;a estropear y <b>habrá </b>que reemplazarlo, al menos que sea barato no?</li><li><i>Flexible</i>: Si sabemos que el hardware se va a estropear durante la ejecución, Hadoop debe estar preparado para soportarlo. Vamos a ver un poco más adelante más detalles de esto, pero para empezar diré que el sistema de ficheros tiene réplicas de los ficheros (por defecto 3), por lo tanto si una máquina se estropea durante la ejecución no pasa nada, el fichero está en 2 otras máquinas, Hadoop se va a enterar, va a clasificar la máquina como "averiada" y va a continuar el job en otra parte.&nbsp;</li></ol><span class="Apple-style-span" style="font-size: large;">Qué es Big Data?&nbsp;</span><br />He insistido bastante en los "muchos datos", cuánto es exactamente "muchos datos"? pues es tan fácil como: "BIG DATA es cuando la cantidad de datos es un problema".<br /><br />Para alguien serán 100 gb, para alguien serán 1Tb y para algun otro seran petabytes.<br /><br />Vamos a profundizar un poco más con el tema del "problema". Yo personalmente creo que hay 3:<br /><ol><li><i>Tiempo</i>: Las herramientas que uso funcionan bien con esta cantidad de datos, pero tarda mucho. Vamos a imaginar que tenemos una base de datos sobre la qual hay que correr una serie de procesos, el departamento de márketing necesita datos actualizados cada 10 horas. Si el proceso finaliza, pero tarda 15 horas tenemos un problema.</li><li><i>Las herramientas dejan de funcionar</i>: Tenemos un programa que funciona bien, crecen los datos y la herramienta cada vez va más lenta, pero no es un problema. Llega un momento que la herramienta simplemente de queda congelada con el input de datos. Tenemos otro problema.&nbsp;</li><li><i>Los datos no caben en la máquina:</i> Tenemos 1 Terabyte (o lo que sea) de datos a tratar y simplemente los datos no caben en la máquina y hay que moverlos a trozos por la red.&nbsp;</li></ol><br /><span class="Apple-style-span" style="font-size: large;">Worflow de trabajo en Hadoop.&nbsp;</span><br />Ya sabemos qué hace Hadoop y ya sabemos cuándo tiene sentido empezar a utilizarlo. Ahora bien, cómo se trabaja?<br />Hadoop consta de dos partes (como veremos en más detalle en el siguiente punto), un motor de map/reduce y un sistema de ficheros distribuido. Lo más importante en este punto es que nos imaginemos a Hadoop como una caja negra que es capaz de <b>almacenar</b>&nbsp;y <b>transformar</b>&nbsp;datos. Por lo tanto para trabajar lo que tenemos que hacer es:<br /><ol><li>Poner los datos nuevos en Hadoop (en caso de que haya): Ponemos los logs del día, los usuarios que se han dado de alta hoy, los eventos del día, etc, en el sistema de ficheros.&nbsp;</li><li>Tratar los datos con un programa map/reduce.&nbsp;</li><li>Sacar los datos del sistema de ficheros para ponerlos a un lugar dónde sean útiles. Vamos a recordar que de momento Hadoop es una caja negra, no es fácil que los del departamento de márketing usen los datos que hay en Hadoop, así que hay que sacarlos de allí y ponerlos en una base de datos, una nosql, una página web, lo que sea.</li></ol><br /><span class="Apple-style-span" style="font-size: large;">Cómo funciona Hadoop? (Map/Reduce y su amigo el DFS)</span><br />Hemos dicho que Hadoop <b>almacena</b>&nbsp;y <b>transforma</b>&nbsp;datos, almacena con un sistema de ficheros y trata los datos con un motor de map/reduce, vamos a verlos:<br /><br /><b>(H)DFS: Hadoop Distributed File System.</b><br />Vamos a empezar por el sistema de ficheros distribuido. Vamos a dejar claro que un sistema de ficheros distribuido es un tema muy serio y complejo. Nadie quiere poner sus ficheros en un sistema experimental con el riesgo de que desaparezcan ficheros, se corrompan o sean inaccesibles, por lo tanto, el principal requerimiento es su estabilidad y robustez.<br /><br />El HDFS fue diseñado a partir del paper del <i>Google File System</i>, no me voy a liar mucho con la explicación de cómo funciona, pero voy a comentar algunas de sus características más importantes:<br /><ul><li>Simple by design: Aunque internamente un sistema de ficheros es muy complejo, el HDFS ha sido diseñado y implementado con un conjunto muy básico y limitado de funcionalidades. Así que no nos podemos esperar grandes "virguerías", al menos a nivel de usuario.</li><li>Robusto y replicado: Es robusto, recordemos que Hadoop está diseñado para ejecutarse en redes de hardware que puede estropearse en cualquier momento. El sistema de ficheros debe ser capaz de continuar trabajando efectivamente hasta un cierto nivel de error tolerable. Una de las características más importantes es que los ficheros están partidos por bloques, y que cada bloque está replicado 3 veces en el clúster, así que si una máquina cae o un disco duro se estropea no pasa nada, aún tenemos dos copias del bloque del fichero en la red.</li><li>Optimizado para Big Data: Los bloques son de 64 mb por defecto, por lo tanto, optimizado para la lectura y escritura de volumenes de datos grandes.</li><li>Escalable: El sistema debe escalar horizontalmente, si necesitas más espacio es tan fácil como poner más máquinas o ampliar los discos duros. El máster se va a encargar de repartir los bloques entre los nuevos nodos de forma transparente.&nbsp;</li><li>Transparente: Hemos dicho que el sistema es sencillo de cara al usuario y que es robusto y escalable, que es tolerante a fallos y que balancea su carga automáticamente y además, sin que el usuario se entere de lo que está pasando. El nodo máster se encarga de todo de forma totalmente transparence. En caso de que una máquina se estropee, el máster va a encargarse de ponerla en una lista negra y va a mirar qué bloques de ficheros contenía, y va a replicarlos (cogiéndolos de otras máquinas) hasta que el número de replicas vuelva a ser el deseado.</li></ul><div>A nivel un poco más técnico, todo el sistema de ficheros está controlado por un nodo <i>máster</i>&nbsp;que se llama el NameNode, podéis ver más información <a href="http://hadoop.apache.org/common/docs/current/hdfs_design.html">aquí</a>.</div><br /><b>Map/Reduce</b><br />El motor de map/reduce es la parte que realiza los cálculos y transformaciones sobre los datos. Básicamente se trata de una serie de componentes software que ejecutan un programa, programado en Java (o alguna otra alternativa que veremos más tarde) que sigue el model de programación del mismo nombre (map/reduce).<br /><br />Bien, pero qué es esto del map/reduce? pues se trata de un esquema de programación paralela que tiene sus orígenes en la programación funcional. Encontraréis mucha más información por la red, pero lo básico y lo importante ahora mismo es entender un poco el concepto, que es bastante sencillo.<br /><br />Tenemos un problema, A, este problema es muy grande y no se puede tratar de forma individual, por lo tanto vamos a coger una función, a la que llamaremos mapper y la vamos a aplicar a trozos de A, de forma que tendremos:<br /><blockquote>&nbsp;A<span class="Apple-style-span" style="font-size: xx-small;">1 </span>--&gt;<span class="Apple-style-span" style="font-size: xx-small;"> </span><b>Mapper</b><span class="Apple-style-span" style="font-size: xx-small;"> </span>--&gt;<span class="Apple-style-span" style="font-size: xx-small;"> </span>A'<span class="Apple-style-span" style="font-size: xx-small;">1</span>&nbsp;&nbsp;</blockquote><blockquote><span class="Apple-style-span" style="font-size: small;">&nbsp;A</span><span class="Apple-style-span" style="font-size: xx-small;">2&nbsp;</span>--&gt;<span class="Apple-style-span" style="font-size: xx-small;">&nbsp;</span><span class="Apple-style-span" style="font-size: small;"><b>Mapper</b></span><span class="Apple-style-span" style="font-size: xx-small;">&nbsp;</span>--&gt;&nbsp;<span class="Apple-style-span" style="font-size: small;">A'</span><span class="Apple-style-span" style="font-size: xx-small;">2</span></blockquote><blockquote>&nbsp; ...<br />&nbsp;A<span class="Apple-style-span" style="font-size: xx-small;">x&nbsp;</span>--&gt;<span class="Apple-style-span" style="font-size: xx-small;">&nbsp;</span><b>Mapper</b><span class="Apple-style-span" style="font-size: xx-small;">&nbsp;</span>--&gt;<span class="Apple-style-span" style="font-size: xx-small;">&nbsp;</span>A'<span class="Apple-style-span" style="font-size: xx-small;">x</span></blockquote><br />Ya tenemos parte del problema resuelto, pero en trozos, ahora toca aplicar el reducer, que es otra función que sabe interpretar y juntar los pequeños resultados que nos ha dado el mapper. De tal modo que:<br /><blockquote>[A'<span class="Apple-style-span" style="font-size: xx-small;">1</span>, A'<span class="Apple-style-span" style="font-size: xx-small;">2</span>, ... , A'<span class="Apple-style-span" style="font-size: xx-small;">x</span>] --&gt; <b>Reducer</b> --&gt; Resultado.</blockquote>No es más que aplicar el 'divide y vencerás' sobre un fichero muy grande. Lógicamente no todos los problemas se pueden resolver con este modelo de programación, es por esto que cale ver si Hadoop es la mejor solución antes de lanzarnos a crear un proyecto.<br /><br /><br /><span class="Apple-style-span" style="font-size: large;">Ejemplos de código:&nbsp;</span><br />En el taller vimos un par de ejemplos (muy básicos y sencillos cabe decir). Están colgados en <a href="https://github.com/lant/ejercicios_hadoop">github</a> y bastante comentados. Otro recurso para tutoriales de programación Java en Hadoop en su <a href="http://hadoop.apache.org/common/docs/current/mapred_tutorial.html">página oficial</a>, bastante más completo.<br /><br /><span class="Apple-style-span" style="font-size: large;">Clúster virtual en EC2 vs Clúster local.&nbsp;</span><br />Tengo que admitir que soy bastante novato en este aspecto. Siempre he tenido la suerte de trabajar con un <a href="http://www.flickr.com/photos/lastfm/tags/datacenter/">clúster dedicado</a>&nbsp;así que eché una ojeada rápida para crear un pequeño clúster virtual en EC2. Según he visto hay 3 possibilidades:<br /><ol><li>Pico y pala: Creas los nodos manualmente y te instalas Hadoop.&nbsp;</li><li>Scripts en el src de Hadoop.</li><li><a href="http://whirr.apache.org/">Apache Whirr</a>.</li></ol><div>Lógicamente escogí la más rápida, en este caso Whirr, que es un proyecto que trata de facilitarnos la creación de clústeres en plataformas de virtualización. El proyecto es bastante joven y aún le falta, pero ya se puede utilizar con unos resultados muy satisfactorios. Con este simple fichero de configuración pude crear un cluster en amazon:&nbsp;</div><div><blockquote><span class="Apple-style-span" style="font-family: 'Courier New', Courier, monospace;">whirr.cluster-name=hadoop<br />whirr.instance-templates=<br />&nbsp; 1 hadoop-namenode+hadoop-jobtracker,<br />&nbsp; 2 hadoop-datanode+hadoop-tasktracker<br />whirr.provider=aws-ec2<br />whirr.identity=*************<br />whirr.credential=***********<br />whirr.hardware-id=c1.xlarge<br />whirr.image-id=us-east-1/ami-da0cf8b3<br />whirr.location-id=us-east-1</span></blockquote>Creo que es bastante autoexplicativo, pero podéis encontrar más detalles en su página web.<br /><br />La clara ventaja sobre los clústeres virtuales sobre los clústeres físicos es claramente la inversión inicial ya que no hay que comprar las máquinas (y el espacio en el datacenter), ni pagar mensualmente la electricidad + mantenimiento, bla bla. Aunque sí que tengo que decir que he oído que el rendimiento de un clúster dedicado es mucho mayor a la que nos encontraremos utilizando un clúster virtual. Del orden de 10 veces más rápido, aunque este número tendría que verse de forma un poco más "científica", cómo dije, esto es un rumor que me dijo un usuario de EC2 que se pasó a clúster dedicado.<br /><br /></div><div><span class="Apple-style-span" style="font-size: large;">Ecosistema de Hadoop</span><br />Hasta ahora he hablado del <i>core </i>de Hadoop. Como se ha visto es muy potente, pero es bastante espartano y no da muchas facilidades amigables al usuario o programador. No obstante el proyecto tuvo una gran adopción, debido a que era software libre y a que básicamente era lo único disponible. Esto causó que muchas empresas pusieran recursos para mejorar el proyecto y crear pequeños proyectos auxiliares que con el tiempo se han convertido en partes importantes de un ecosistema muy activo.<br /><br />Podéis encontrar más información sobre los proyectos que considero más interesantes en un post que ya escribí en este mismo <a href="http://distribuint.blogspot.com/2010/09/el-ecosistema-hadoop.html">blog</a>.<br /><br /><span class="Apple-style-span" style="font-size: large;">Preguntas?</span><br />Esto ha sido una pequeña introducción a Hadoop, muy a <i>grosso modo</i>&nbsp;y sin entrar en detalles peliagudos. Como siempre, si tenéis alguna duda no dudéis en preguntarla en la sección de comentarios o bien contactando conmigo.<br /><br /></div>