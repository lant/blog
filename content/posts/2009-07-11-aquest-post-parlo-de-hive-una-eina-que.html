---
layout: post
title: Hive
date: '2009-07-11T20:52:00.005+01:00'
author: Marc de Palol
tags:
- hadoop
- hive
modified_time: '2009-07-12T15:58:32.415+01:00'
blogger_id: tag:blogger.com,1999:blog-6541464271719475452.post-8486906221087052202
blogger_orig_url: http://distribuint.blogspot.com/2009/07/aquest-post-parlo-de-hive-una-eina-que.html
---

Aquest post parlo de Hive, una eina que han fet a Facebook que converteix un llenguatge d'accés a dades semblant SQL a treballs map-reduce per fer consultes als directoris guardats en el DFS de Hadoop.<br /><br />Un dels problemes que tenim els usuaris de Hadoop és que estem treballant amb uns volums de dades considerables. En cas contrari seria millor utilitzar Python o Erlang o una base de dades relacional (insisteixo una vegada més, Hadoop si i només si s'ha de treballar amb moltes dades). Com ja he explicat en <a href="http://distribuint.blogspot.com/2009/03/hadoop.html">altres</a> posts un dels sub-projectes de Hadoop és el sistema de fitxers, fet a inspiració del Google File System (GFS). És un sistema redundant i distribuït, per tant cada fitxer està fragmentat per la xarxa i n'hi ha diverses còpies. Això implica que sigui molt difícil perdre un fitxer (ja que n'hi ha 3 còpies diferents), però al mateix moment es fa més complicat poder accedir a aquest fitxer, ja que qualsevol manipulació que se li vulgui aplicar (sense un treball map/reduce de Hadoop) força que s'hagi de baixar del clúster, i aquests fitxers poden arribar a ser molt grans (molt molt grans). <br /><br />Això a efectes pràctics significa que no es pot fer de forma senzilla un 'grep' per buscar continguts. El fitxer està repartit pel clúster per tant, s'ha de fer un treball map-reduce per fer el grep. El codi de hadoop ja porta implementada aquesta utilitat, però igualment és incòmode i poc potent. <br /><br />En principi no hauria de ser un problema, ja que normalment interessa processar una entrada i un cop tenim la sortida a punt la baixem tota del clúster per servir-la a la pàgina web, per inserir-la a la base de dades o el que sigui. El problema ve quan necessites saber (urgentment) una certa línia d'un fitxer de 200 Gb que està fragmentat dins el clúster.<br /><br />Potser això és una mica difícil d'entendre, un exemple:<br /><br />Com sabeu fa poc es va morir Michael Jackson, doncs bé, quan la noticia es va escampar la gent va començar a escoltar la seva música. A last.fm vam començar a rebre <i>scrobbles</i> de Michael Jackson, aquests <i>scrobbles</i>, que basicament hi ha l'identificador d'usuari i la cançó que aquest ha escoltat es guarden al sistema de fitxers distribuït per a ser processats per Hadoop. No ens volíem esperar a que fos al vespre per saber quanta gent de més havia escoltat al difunt rei del pop. La solució era fer un programa (en Java o bé en Dumbo) per agafar aquestes dades i fer un grep per veure quants usuaris era, una cosa que utilitzant bash senzillament seria:<br /><pre><br />bash $> grep michaeljackonid fitxer_scrobbles | sort -k columna_usuari -u | wc -l <br /></pre><br />és en Java molt més pesat i complicat d'escriure.<br /><br />Els de l'equip de dades ja havíem tingut diverses peticions d'aquestes anteriorment, i ja tenim una sèrie de programes (mal fets, a base de copy and paste, ràpids) per fer consultes. El principal problema és que d'aquesta manera s'acaba tinguent una bateria d'scripts de consulta que són impossibles de reutilitzar, totalment específics i que serveixen tant sols una vegada. <br /><br />Però aquesta vegada va ser diferent, perquè des de feia uns dies estàvem jugant amb Hive!<br /><br />Segons la <a href="http://wiki.apache.org/hadoop/Hive">seva pàgina</a>, Hive és una infraestructura de 'warehouse' (magatzem de dades) que corre sobre Hadoop i permet la consulta de dades. Per tant, és en certa manera. Una infraestructura per córrer SQL sobre directoris de DFS.<br /><br />Convé tenir en consideració els següent punt, <b>molt important</b>:<br /><ul><br /> <li>Hive <b>no</b> és una base de dades.<br /></ul><br /><br />El que fa Hive és tenir en uns fitxers propis del DFS de Hadoop una sèrie de dades que en certa manera descriuen les estructures de directoris dels directoris normals del DFS. Un cop tenim això, la línia de comandaments de Hive permet fer consultes amb SQL. S'ha elegit aquest llenguatge de consulta perquè (<i>desgraciadament?</i>) és el que sap tothom i realment no fa falta crear-ne cap altre. <br /><br />Aquesta consulta SQL és convertida per Hive en diversos passos, com podem veure en el <a href="http://wiki.apache.org/hadoop/Hive/Design">document d'arquitectura</a> en una sèrie de tasques Map Reduce que corren de forma normal al clúster.<br /><br />Aquestes tasques senzillament agrupen les dades i les filtren tal com hem dit a la consulta, i en cas que sigui necessari les ordenen (<i>order by</i>), treuen duplicats (<i>distinct</i>)...<br /><br />Per tant, amb Hive podem córrer una consulta sobre totes les dades del sistema de fitxers distribuït, de forma senzilla i sense haver de carregar totes les dades en una base de dades relacional.<br /><br />A mi personalment, m'estalviarà moltes hores, ah, i per cert, l'augment d'oients de Michael Jackson el podeu trobar <a href="http://www.flickr.com/photos/lastfm/3661753675/">aquí</a>.