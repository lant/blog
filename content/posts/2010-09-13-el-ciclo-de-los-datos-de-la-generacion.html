---
layout: post
title: El ciclo de los datos, de la generación al uso.
date: '2010-09-12T23:50:00.000+01:00'
author: Marc de Palol
tags:
- hbase
- hadoop
- big data
- nosql
modified_time: '2010-09-12T23:50:18.592+01:00'
blogger_id: tag:blogger.com,1999:blog-6541464271719475452.post-4928615593741829463
blogger_orig_url: http://distribuint.blogspot.com/2010/09/el-ciclo-de-los-datos-de-la-generacion.html
---

Trabajar con grandes cantidades de datos es complicado.&nbsp;Muchas empresas están incorporando&nbsp;Map/Reduce, básicamente su implementación libre, Hadoop para el tratamiento de sus datos. El ecosistema de herramientas de Hadoop va creciendo también, y cada vez es más complejo, ya que tiene que solucionar muchos casos, pero aún le falta alguna pieza para completar el ciclo total de los datos.<br /><br />Con "ciclo total de los datos" me refiero a las diferentes fases del proceso, en Hadoop normalmente:<br /><ol><li><b>Ingestión de datos en el sistema de ficheros: </b>Hadoop trabaja sobre DFS (normalmente), así pues tenemos que enviar los datos al clúster.</li><li><b>Tratamiento de datos:</b> Operaciones Map/Reduce en Hadoop.</li><li><b>Visualización o acceder a los datos:</b> Una vez tenemos los resultados de los trabajos, hay que sacar los datos del DFS para poder presentarlos.</li></ol>El primer punto es bastante interesante. Hadoop es útil cuando trabajamos con muchos datos (&gt;Gbs), ahora bien, los datos deben estar en el DFS, esto puede ser un problema. El framework ofrece diferentes comandas para insertar ficheros, en mi caso un poco de scripting combinado con estos comandos siempre ha sido suficiente. Aunque estoy empezando a considerar usar <a href="http://github.com/cloudera/flume">Flume</a>, un proyecto de <a href="http://www.cloudera.com/">Cloudera</a> muy interesante que inserta datos en el sistema de ficheros distribuido como si fueran Pipes de Unix.<br /><br />El segundo punto es bastante sencillo, ya que es el nucleo de Hadoop. Una vez finalizado estamos en el tercer punto, algunas veces el volumen de datos de salida será mucho inferior respecto a la entrada, a veces similar, y otras muy mayor.<br /><br />Lógicamente nos interesan estos datos de salida, y la mayoría de veces necesitaremos sacarlos del DFS para poderlos estudiar, visualizar, o como es mi caso, servir a través de un servicio web.<br /><br />Por qué hay que sacar los datos del DFS? por qué no los puedo usar desde allí?<br /><br />Básicamente el problema viene del diseño del DFS en sí mismo, el sistema de ficheros está optimizado para los trabajos de Hadoop, que leen sequencialmente todo el fichero, esto significa que el sistema es muy lento leyendo posiciones aleatorias en un fichero. A efectos prácticos esto significa que haya mucha latencia y que no sea factible servir datos de forma rápida.<br /><br />La solución más viable si no queremos (o no podemos) mover los datos es usar <a href="http://hbase.apache.org/">HBase</a>, aunque de momento no es considerado como la mejor opción para servir las peticiones de una página web con un volumen importante de datos, aunque la comunidad está poniendo muchos esfuerzos para mejorar este tipo de rendimiento (lectura en posiciones aleatorias de ficheros) así como la estabilidad. Pero bueno, esto es otra historia.<br /><br />Así pues, qué se hace para solucionar este tercer punto?<br /><br />Lo más común es volcar los contenidos de los ficheros de DFS a una base de datos o a una K/V Store. Aunque tampoco es trivial.<br /><br />El primer caso, sacar los ficheros y insertar el contenido a una RDBMS, &nbsp;caso plantea unos problemas bastante interesantes, básicamente se pueden hacer dos cosas:<br /><ul><li><b>Meterle caña a la base de datos: </b>Una vez tenemos los ficheros del DFS se pueden convertir a SQL, luego se sube este fichero a la base de datos (con un <i>copy</i><b>)</b>, se crea una tabla nueva y se hace un swap. El problema? pues que si la nueva tabla ocupa 90 Gbytes y lo tienes que hacer cada día a lo mejor el administrador de la base de datos te viene chillando como un poseso (comprobado).</li><li><b>Insertar sólo las deltas: </b>Esta solución puede parecer más diplomática. Se trata de calcular (si se puede) las diferencias que hay entre la salida de Hadoop con el contenido de la base de datos, el resultado será una bateria de <i>inserts</i>, otra de <i>updates</i> y otra de <i>deletes</i>. Esta solución no es senzilla programáticamente y tiene el problema que hay que volcar los contenidos de la base de datos a Hadoop para que se pueda ver que ha cambiado.&nbsp;</li></ul><div>Nótese que en ningún momento estoy recomendando hacer las operaciones contra la base de datos desde los reducers de Hadoop, a no ser que queramos hacer un DDoS a nuestra base de datos (o que odiemos al DBA).</div><div><br /></div><div>El segundo caso (utilizar una K/V Store en vez de una base de datos relacional) dependerá mucho de la K/V Store que utilizemos, pero la idea es la misma, podemos intentar actualizar los datos desde Hadoop y arriesgarnos a que se caiga todo o crear un fichero para que la K/V lo lea, en este caso jugamos un poco con ventaja ya que el movimiento NoSQL ha ido muy ligado al movimento Hadoop. Esto significa que muchas bases de datos no relacionales ya tienen un "conector".&nbsp;</div><div><br /></div><div>Algunos ejemplos:</div><ul><li><b><a href="http://cassandra.apache.org/">Apache Cassandra</a>: </b>Se puede utilizar un <i>OutputFormat</i>&nbsp;especial para que la salida de los jobs de Hadoop sea totalmente compatible con los ficheros de almacenamiento que usa Cassandra, asi pues, sólo se trata de crear estos ficheros, enviarlos a los nodos de Cassandra para que los lean. Feature un poco experimental a día de hoy.</li><li><b><a href="http://www.mongodb.org/">MongoDB</a>: </b>También hay un <i>OutputFormat</i>&nbsp;, en este caso escribe el resultado del tratamiento en Hadoop en BSON. El formato no forma parte de la distribución de MongoDB a día de hoy, está en un proyecto en <a href="http://github.com/novus/luau">github</a>&nbsp;(no he podido probarlo aún, pero me muero de ganas).&nbsp;</li><li><b><a href="http://project-voldemort.com/">Voldemort</a>: </b>Fue creada por los chicos de LinkedIn justamente para solucionar este problema, tiene soporte de Hadoop por defecto.</li><li><b><a href="http://hbase.apache.org/">HBase</a>: </b>Aunque he dicho que no es la mejor solución para servir datos <i>online</i>&nbsp;cabe decir que tiene soporte de leer ficheros generados por Hadoop por defecto.</li></ul>Recientemente he estado trabajando mucho en este tema y he estado modificando un servicio web para solucionar en parte este problema. Voy a presentarlo en la séptima reunión del <a href="http://huguk.org/">HUGUK</a>&nbsp;el próximo 7 de noviembre (creo), espero poder colgar las transparencias y hacer un post aquí cuando lo tenga todo listo.<br /><br />Mientras, hay alguien que tenga alguna sugerencia para solucionar todo este follón ?